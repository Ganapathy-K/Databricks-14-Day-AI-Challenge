{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e7e2547-c606-4ea0-aa22-bc7a1ff67a97",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Load Events Data and Save as Delta Table in Workspace"
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.format(\"delta\").load(\"/Volumes/workspace/ecommerce/silver/events_delta\")\n",
    "df.write.mode(\"overwrite\").saveAsTable(\"workspace.default.ecommerce_events\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf654cd5-0eca-43a8-be93-8805c6170b15",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Count of Ecommerce Events in Workspace Table"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>COUNT(*)</th></tr></thead><tbody><tr><td>67401460</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         67401460
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "COUNT(*)",
            "nullable": false,
            "type": "long"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 9
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "COUNT(*)",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SELECT COUNT(*) FROM workspace.default.ecommerce_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0392b19f-ea79-41b3-a424-602883e5770a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create Text Column"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+\n|event_type|                text|\n+----------+--------------------+\n|      view|neoline auto.acce...|\n|      view|furniture.bedroom...|\n|      view|            cordiant|\n+----------+--------------------+\nonly showing top 3 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import concat_ws, col\n",
    "nlp_df = spark.table(\"workspace.default.ecommerce_events\").select(\"event_type\", concat_ws(\" \", col(\"brand\"), col(\"category_code\")).alias(\"text\")).dropna()\n",
    "nlp_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c27d2580-2d9f-488e-bfc8-50d0a635c6c9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Text → Tokens → TF-IDF Features"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, HashingTF, IDF, StringIndexer\n",
    "label_indexer = StringIndexer(inputCol=\"event_type\", outputCol=\"label\")\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "tf = HashingTF(inputCol=\"words\", outputCol=\"raw_features\", numFeatures=1000)\n",
    "idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bad04eb0-97d9-4811-ba01-71a4f39fe9fa",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Simple NLP Classifier"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\", maxIter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd8fc64a-55cb-428b-ac28-1e3ac7bf659b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Build Spark NLP Pipeline"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages=[label_indexer, tokenizer, tf, idf, lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "489e4546-7cb3-4ed6-b2e0-54a4471e8730",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Train / Test Split + Model Fit"
    }
   },
   "outputs": [],
   "source": [
    "train_df, test_df = nlp_df.randomSplit([0.8, 0.2], seed=42)\n",
    "model = pipeline.fit(train_df)\n",
    "preds = model.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "589e6b18-1a1d-4cf4-bfde-341c3b667eb2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Evaluate NLP Model"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLP Classification Accuracy: 0.9429696462781593\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(preds)\n",
    "print(\"NLP Classification Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dffe28c7-a5d1-40ff-b100-26296d7434af",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Predicting Multiple Classes"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+--------+\n|event_type|prediction|   count|\n+----------+----------+--------+\n|  purchase|       0.0|  183039|\n|      cart|       0.0|  585650|\n|      view|       0.0|12710299|\n+----------+----------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "preds.groupBy(\"event_type\", \"prediction\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d536f94-565d-4274-b9c8-9db56e8ea5b8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Convert Back to Text Labels"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+---------------+\n|text|event_type|predicted_event|\n+----+----------+---------------+\n|    |cart      |view           |\n|    |cart      |view           |\n|    |cart      |view           |\n|    |cart      |view           |\n|    |cart      |view           |\n|    |cart      |view           |\n|    |cart      |view           |\n|    |cart      |view           |\n|    |cart      |view           |\n|    |cart      |view           |\n+----+----------+---------------+\nonly showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import IndexToString\n",
    "converter = IndexToString(inputCol=\"prediction\", outputCol=\"predicted_event\", labels=model.stages[0].labels)\n",
    "final_preds = converter.transform(preds)\n",
    "final_preds.select(\"text\",\"event_type\",\"predicted_event\").show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b38f5c2-6e6d-496c-ac1a-983b393a8d4a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Conversion Funnel Metrics"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+\n|event_type|     cnt|\n+----------+--------+\n|  purchase|  916930|\n|      cart| 2930018|\n|      view|63554512|\n+----------+--------+\n\nView → Cart rate: 0.04610243880088325\nCart → Purchase rate: 0.31294346997185685\nInsight: Low add-to-cart rate — product pages need improvement.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, count, when\n",
    "funnel = (df.groupBy(\"event_type\").agg(count(\"*\").alias(\"cnt\")))\n",
    "funnel.show()\n",
    "# Compute conversion rates\n",
    "views = df.filter(col(\"event_type\")==\"view\").count()\n",
    "carts = df.filter(col(\"event_type\")==\"cart\").count()\n",
    "purchases = df.filter(col(\"event_type\")==\"purchase\").count()\n",
    "view_to_cart = carts / views\n",
    "cart_to_purchase = purchases / carts\n",
    "print(\"View → Cart rate:\", view_to_cart)\n",
    "print(\"Cart → Purchase rate:\", cart_to_purchase)\n",
    "# Auto Insight Text\n",
    "if view_to_cart < 0.05:\n",
    "    print(\"Insight: Low add-to-cart rate — product pages need improvement.\")\n",
    "else:\n",
    "    print(\"Insight: Healthy add-to-cart ratio observed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a87930d-465d-45c5-8dd0-38d7d4946682",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Top Brands Driving Revenue"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+\n|  brand|      revenue|\n+-------+-------------+\n|  apple|127512524.880|\n|samsung| 54869650.970|\n| xiaomi| 11259845.910|\n+-------+-------------+\nonly showing top 3 rows\nInsight: Brand 'apple' contributes the highest purchase revenue.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum, round\n",
    "from pyspark.sql.types import DecimalType\n",
    "# brand_rev = (df.filter(col(\"event_type\") == \"purchase\").groupBy(\"brand\").agg(round(sum(\"price\"), 3).alias(\"revenue\")).orderBy(col(\"revenue\").desc()))\n",
    "brand_rev = (df.filter(col(\"event_type\") == \"purchase\").groupBy(\"brand\").agg(sum(\"price\").cast(DecimalType(18,3)).alias(\"revenue\")).orderBy(col(\"revenue\").desc()))\n",
    "brand_rev.show(3)\n",
    "# Generate Insight Statement\n",
    "top_brand = brand_rev.first()[\"brand\"]\n",
    "print(f\"Insight: Brand '{top_brand}' contributes the highest purchase revenue.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "030fc359-7d60-499b-a2f1-c4af4aeadb1f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Time Pattern Insight"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>event_hour</th><th>count</th></tr></thead><tbody><tr><td>16</td><td>4504001</td></tr><tr><td>15</td><td>4443369</td></tr><tr><td>17</td><td>4410648</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         16,
         4504001
        ],
        [
         15,
         4443369
        ],
        [
         17,
         4410648
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "event_hour",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "count",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insight: User activity peaks around hour 16.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import hour\n",
    "hourly = (df.withColumn(\"event_hour\", hour(\"event_time\")).groupBy(\"event_hour\").count().orderBy(\"count\", ascending=False))\n",
    "display(hourly.limit(3))\n",
    "\n",
    "peak_hour = hourly.first()[\"event_hour\"]\n",
    "print(f\"Insight: User activity peaks around hour {peak_hour}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "293ef599-5457-4675-8c3b-d319189f9dc9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Behavior Segment Insight"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Users with many views but no purchases: 522706\nInsight: There is a segment of users who browse heavily but do not purchase.\n"
     ]
    }
   ],
   "source": [
    "user_behavior = (df.groupBy(\"user_id\").agg(count(when(col(\"event_type\")==\"view\", True)).alias(\"views\"), count(when(col(\"event_type\")==\"purchase\", True)).alias(\"purchases\")))\n",
    "high_view_low_buy = user_behavior.filter((col(\"views\") > 20) & (col(\"purchases\")==0)).count()\n",
    "print(\"Users with many views but no purchases:\", high_view_low_buy)\n",
    "\n",
    "if high_view_low_buy > 0:\n",
    "    print(\"Insight: There is a segment of users who browse heavily but do not purchase.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a867d07f-45c0-4b7c-8acc-8747f41dbb58",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "AI Insight from ML Prediction"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+\n|   brand|purchase_probability|\n+--------+--------------------+\n| respect|0.015004425522238758|\n|  xiaomi| 0.01500386899698927|\n|cordiant|0.015001611207727705|\n| lucente|0.014992213486940444|\n|  huawei|0.014991325389491477|\n+--------+--------------------+\nonly showing top 5 rows\nAI Insight: Brand 'respect' shows highest predicted purchase probability (0.015).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import when, col, avg\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "# 1️⃣ Create label: purchase = 1 else 0\n",
    "df_ai = df.withColumn(\"label\", when(col(\"event_type\") == \"purchase\", 1).otherwise(0))\n",
    "\n",
    "# 2️⃣ Small feature set for fast training\n",
    "brand_indexer = StringIndexer(inputCol=\"brand\", outputCol=\"brand_index\", handleInvalid=\"keep\")\n",
    "assembler = VectorAssembler(inputCols=[\"price\", \"brand_index\"], outputCol=\"features\")\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\", maxIter=10)\n",
    "pipeline = Pipeline(stages=[brand_indexer, assembler, lr])\n",
    "\n",
    "# 3️⃣ Train / test split\n",
    "train, test = df_ai.randomSplit([0.8, 0.2], seed=42)\n",
    "model = pipeline.fit(train)\n",
    "preds = model.transform(test)\n",
    "\n",
    "# 4️⃣ Extract purchase probability safely (vector → scalar)\n",
    "get_purchase_prob = udf(lambda v: float(v[1]), DoubleType())\n",
    "preds2 = preds.withColumn(\"purchase_prob\", get_purchase_prob(col(\"probability\")))\n",
    "\n",
    "# 5️⃣ Aggregate into AI insight metric\n",
    "brand_scores = (preds2.groupBy(\"brand\").agg(avg(\"purchase_prob\").alias(\"purchase_probability\")).orderBy(col(\"purchase_probability\").desc()))\n",
    "brand_scores.show(5)\n",
    "\n",
    "# 6️⃣ Auto AI insight statement\n",
    "top_row = brand_scores.orderBy(col(\"purchase_probability\").desc()).first()\n",
    "brand_name = top_row[\"brand\"]\n",
    "prob_value = float(top_row[\"purchase_probability\"])\n",
    "print(\"AI Insight: Brand '{}' shows highest predicted purchase probability ({:.3f}).\".format(brand_name, prob_value))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5052745462194377,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Databricks 14-Day AI Challenge - Day 14",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}