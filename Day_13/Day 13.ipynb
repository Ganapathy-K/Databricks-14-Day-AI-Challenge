{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8d85062-f7f6-47bc-8b16-d9a836d5882f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Importing PySpark Libraries for Regression Modeling"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='dbfs:/databricks/mlflow-tracking/972906041565883', creation_time=1769888538803, experiment_id='972906041565883', last_update_time=1769891661645, lifecycle_stage='active', name='/Shared/ecommerce_price_experiment', tags={'mlflow.experiment.sourceName': '/Shared/ecommerce_price_experiment',\n",
       " 'mlflow.experimentType': 'MLFLOW_EXPERIMENT',\n",
       " 'mlflow.ownerEmail': 'ganapathisking@gmail.com',\n",
       " 'mlflow.ownerId': '78221769891557'}>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression, GeneralizedLinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
    "from mlflow.models.signature import infer_signature\n",
    "\n",
    "import os\n",
    "import mlflow\n",
    "import mlflow.spark\n",
    "import logging\n",
    "\n",
    "logging.getLogger(\"mlflow\").setLevel(logging.ERROR)  # Silence MLflow noise (optional but clean)\n",
    "spark.sql(\"\"\"CREATE VOLUME IF NOT EXISTS workspace.ecommerce.mlflow_tmp\"\"\")  # Create UC volume for MLflow artifacts (required on serverless)\n",
    "os.environ[\"MLFLOW_DFS_TMP\"] = \"/Volumes/workspace/ecommerce/mlflow_tmp\"  # Set MLflow temp dir\n",
    "mlflow.set_experiment(\"/Shared/ecommerce_price_experiment\")  # Set MLflow experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21e7970b-ae0a-4ab3-967d-e6ea0d1a73b8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Loading the E-Commerce Dataset"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------+\n|price|        category_id|\n+-----+-------------------+\n|  842|2053013555631882655|\n|   90|2053013556202308035|\n|  270|2053013561579406073|\n+-----+-------------------+\nonly showing top 3 rows\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"delta\").load(\"dbfs:/Volumes/workspace/ecommerce/silver/events_delta\")\n",
    "df = df.select(col(\"price\").cast(\"int\"), col(\"category_id\").cast(\"string\")).dropna()\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da916e68-f968-488a-ac46-8a92a1033d3c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Splitting E-commerce Data into Training and Test Sets"
    }
   },
   "outputs": [],
   "source": [
    "df_ml = df.select(\"price\", \"category_id\")\n",
    "train_df, test_df = df_ml.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b1fa3ae-6ae9-4de2-a791-874aa41f874a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Transforming Category IDs to Feature Vectors for ML"
    }
   },
   "outputs": [],
   "source": [
    "category_indexer = StringIndexer(inputCol=\"category_id\", outputCol=\"category_index\", handleInvalid=\"keep\")\n",
    "assembler = VectorAssembler(inputCols=[\"category_index\"], outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "657cec88-c6dc-4ae7-9163-e36203242171",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Setting MLflow Temporary Directory for MLflow"
    }
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# display(spark.sql(\"CREATE VOLUME IF NOT EXISTS workspace.ecommerce.mlflow_tmp\"))\n",
    "# os.environ[\"MLFLOW_DFS_TMP\"] = \"/Volumes/workspace/ecommerce/mlflow_tmp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de2c9961-c72b-4276-8fef-cfaa700a0de6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Defining Regression Models with Price as Target Variabl ..."
    }
   },
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"LinearRegression\": LinearRegression(labelCol=\"price\", featuresCol=\"features\"),\n",
    "    \"RidgeRegression\": LinearRegression(labelCol=\"price\", featuresCol=\"features\", regParam=0.1, elasticNetParam=0.0),\n",
    "    \"LassoRegression\": LinearRegression(labelCol=\"price\", featuresCol=\"features\", regParam=0.1, elasticNetParam=1.0)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4bbedf3d-90a4-4ee9-981e-3ab3c6bf1b19",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Initializing RMSE Evaluator for Price Prediction Model"
    }
   },
   "outputs": [],
   "source": [
    "evaluator = RegressionEvaluator(labelCol=\"price\", predictionCol=\"prediction\", metricName=\"rmse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc5b5143-d9ab-49ba-b920-d5f59a182c51",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Train, Log, Compare Multiple Regression Models with MLflow"
    }
   },
   "outputs": [],
   "source": [
    "mlflow.set_experiment(\"/Shared/ecommerce_price_experiment\")\n",
    "results = []\n",
    "for model_name, model in models.items():\n",
    "    with mlflow.start_run(run_name=model_name):\n",
    "        pipeline = Pipeline(stages=[category_indexer, assembler, model])  # 3. Build Spark ML pipeline \n",
    "        fitted_model = pipeline.fit(train_df)  # 1. Train 3 different models (linear regression, ridge regression, lasso regression) through looping \n",
    "        predictions = fitted_model.transform(test_df)\n",
    "        rmse = evaluator.evaluate(predictions)\n",
    "        mlflow.log_param(\"model_type\", model_name)\n",
    "        mlflow.log_metric(\"rmse\", rmse)  # 2. Compare metrics in MLflow \n",
    "        input_example = train_df.select(\"category_id\").limit(5).toPandas()  # Input example (fixes signature warning)\n",
    "        output_example = predictions.select(\"prediction\").limit(5).toPandas()  # Output example (fixes signature warning)\n",
    "        signature = infer_signature(input_example, output_example)  # Signature (fixes MLflow model warnings)\n",
    "        mlflow.spark.log_model(fitted_model, artifact_path=\"model\")\n",
    "        results.append((model_name, rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b1a23e3-8d29-48d4-bdd1-636cb63dbaa5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Selecting the Best Model"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "'LassoRegression'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "348.1669572711918"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_model_name, best_rmse = sorted(results, key=lambda x: x[1])[0]  # 4. Select best model \n",
    "display(best_model_name, best_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34bf6ae6-d259-4a29-ac73-f3229e08566c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Displaying the Best Model Ordered by RMSE"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------------+\n|           model|              rmse|\n+----------------+------------------+\n| LassoRegression| 348.1669572711918|\n| RidgeRegression|348.16696203873227|\n|LinearRegression|348.16696620410744|\n+----------------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.createDataFrame(results, [\"model\", \"rmse\"]).orderBy(\"rmse\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07c2396d-0d56-43cb-8ca1-23b04a6df73f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Retrain Best Model + Register in MLflow Model Registry"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'ecommerce_price_model' already exists. Creating a new version of this model...\nCreated version '2' of model 'workspace.default.ecommerce_price_model'.\n"
     ]
    }
   ],
   "source": [
    "best_estimator = models[best_model_name]\n",
    "with mlflow.start_run(run_name=f\"{best_model_name}_final\"):\n",
    "    final_pipeline = Pipeline(stages=[category_indexer, assembler, best_estimator])\n",
    "    final_model = final_pipeline.fit(train_df)\n",
    "    final_predictions = final_model.transform(test_df)\n",
    "    final_rmse = evaluator.evaluate(final_predictions)\n",
    "    mlflow.log_param(\"best_model\", best_model_name)\n",
    "    mlflow.log_metric(\"rmse\", final_rmse)\n",
    "    input_example = (train_df.select(\"category_id\").limit(10).toPandas().astype({\"category_id\": \"string\"}))  # Build input example (only model inputs, cast safe types)\n",
    "    output_example = (final_predictions.select(\"prediction\").limit(10).toPandas().astype({\"prediction\": \"float64\"}))  # Build output example (only prediction column, cast float)\n",
    "    signature = infer_signature(input_example, output_example)\n",
    "    mlflow.spark.log_model(final_model, artifact_path=\"model\", registered_model_name=\"ecommerce_price_model\", input_example=input_example, signature=signature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0db34459-676c-4679-a0ad-acbf117c8318",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Hyperparameter Tuning for LinReg with Validation"
    }
   },
   "outputs": [],
   "source": [
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"price\")\n",
    "pipeline_lr = Pipeline(stages=[category_indexer, assembler, lr])\n",
    "paramGrid = (ParamGridBuilder().addGrid(lr.regParam, [0.0, 0.01, 0.1]).addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]).build())\n",
    "tvs = TrainValidationSplit(estimator=pipeline_lr, estimatorParamMaps=paramGrid, evaluator=evaluator, trainRatio=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ece278a2-ed55-48c5-a88f-0a36ac2af281",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Configuring SparkML and MLflow Temporary Paths"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temp paths configured\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "spark.sql(\"CREATE VOLUME IF NOT EXISTS workspace.ecommerce.sparkml_tmp\")  # Create UC volume if not already created\n",
    "# Set required temp paths for serverless + MLflow + SparkML\n",
    "os.environ[\"MLFLOW_DFS_TMP\"] = \"/Volumes/workspace/ecommerce/mlflow_tmp\"\n",
    "os.environ[\"SPARKML_TEMP_DFS_PATH\"] = \"/Volumes/workspace/ecommerce/sparkml_tmp\"\n",
    "print(\"Temp paths configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f8ff82e-deba-43bc-8071-3e002564821f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Run Tuning & Log Tuned Model"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.12/site-packages/mlflow/types/utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n  warnings.warn(\nRegistered model 'ecommerce_price_model_tuned' already exists. Creating a new version of this model...\nCreated version '1' of model 'workspace.default.ecommerce_price_model_tuned'.\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run(run_name=\"LinearRegression_Tuned\"): \n",
    "    tvs_model = tvs.fit(train_df)\n",
    "    tuned_model = tvs_model.bestModel\n",
    "    tuned_preds = tuned_model.transform(test_df)\n",
    "    tuned_rmse = evaluator.evaluate(tuned_preds)\n",
    "    mlflow.log_metric(\"rmse\", tuned_rmse)\n",
    "    input_example = train_df.limit(5).toPandas()\n",
    "    output_example = tuned_model.transform(train_df.limit(5)).toPandas()\n",
    "    signature = infer_signature(input_example, output_example)\n",
    "    mlflow.spark.log_model(tuned_model, artifact_path=\"model\", registered_model_name=\"ecommerce_price_model_tuned\", input_example=input_example, signature=signature)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Databricks 14-Day AI Challenge - Day 13",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
