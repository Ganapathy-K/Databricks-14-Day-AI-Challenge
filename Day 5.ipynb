{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3650012b-857b-4532-ba76-a7c7e655d632",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Append new purchase event to Delta table"
    }
   },
   "outputs": [],
   "source": [
    "# Data to be appended to the existing table\n",
    "new_data = [(\"2019-11-01 00:00:00 UTC\", \"view\", \"1003461\", \"2053013555631882655\", \"electronics.smartphone\", \"xiaomi\", \"489.07\", \"520088904\", \"4d3b30da-a5e4-49df-b1a8-ba5943f1dd33\"), \n",
    "            (\"2019-11-01 00:01:15 UTC\", \"view\", \"1003462\", \"2053013555631882655\", \"electronics.smartphone\", \"samsung\", \"699.99\", \"520088905\", \"7a1c42de-9b22-4c6d-9e18-2e9e1d1f4c21\")]\n",
    "# Define the column names\n",
    "columns = [\"event_time\", \"event_type\", \"product_id\", \"category_id\", \"category_code\", \"brand\", \"price\", \"user_id\", \"user_session\"]\n",
    "# Create a DataFrame with the new data\n",
    "incremental_df = spark.createDataFrame(new_data, columns)\n",
    "# Insert the new data into the Delta table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a9d83e2-92a6-44ae-a4cd-9d079137a8a9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Implement incremental MERGE"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate row detected. No insertion performed.\n"
     ]
    }
   ],
   "source": [
    "# Import libary DeltaTable\n",
    "from delta.tables import DeltaTable\n",
    "# Read the Delta table\n",
    "delta_table = DeltaTable.forName(spark, \"workspace.ecommerce.ecommerce_data_2019_nov\")\n",
    "# Perform the merge\n",
    "delta_table.alias(\"target\").merge(\n",
    "    incremental_df.alias(\"source\"),\n",
    "    \"\"\"\n",
    "    target.event_time   = source.event_time AND\n",
    "    target.event_type   = source.event_type AND\n",
    "    target.product_id   = source.product_id AND\n",
    "    target.category_id  = source.category_id AND\n",
    "    target.category_code= source.category_code AND\n",
    "    target.brand        = source.brand AND\n",
    "    target.price        = source.price AND\n",
    "    target.user_id      = source.user_id AND\n",
    "    target.user_session = source.user_session\n",
    "    \"\"\"\n",
    ").whenNotMatchedInsertAll().execute()\n",
    "# Get the latest operation from Delta history\n",
    "history_df = delta_table.history(1)\n",
    "history = history_df.collect()[0]\n",
    "# Extract number of inserted rows\n",
    "row_inserted = int(history.operationMetrics.get(\"numInsertedRows\", 0))\n",
    "# Check if the row was inserted or not\n",
    "if row_inserted > 0:\n",
    "    print(f\"Row successfully inserted. Inserted rows: {row_inserted}\")\n",
    "else:\n",
    "    print(\"Duplicate row detected. No insertion performed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "111da448-f36b-4884-9987-1a90457382fb",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Query historical versions"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version: 0 | Shape: (67501979, 9)\nVersion: 1 | Shape: (67501979, 9)\nVersion: 2 | Shape: (67501979, 9)\nVersion: 3 | Shape: (67501980, 9)\nVersion: 4 | Shape: (67501980, 9)\nVersion: 5 | Shape: (67501980, 9)\nVersion: 6 | Shape: (67501980, 9)\nVersion: 7 | Shape: (67501980, 9)\nVersion: 8 | Shape: (67501980, 9)\nVersion: 9 | Shape: (67501981, 9)\nVersion: 10 | Shape: (67501981, 9)\nVersion: 11 | Shape: (67501981, 9)\n"
     ]
    }
   ],
   "source": [
    "# Query historical versions of the Delta table\n",
    "for v in range(delta_table.history().count()):\n",
    "    df_version = spark.read.format(\"delta\").option(\"versionAsOf\", v).table(\"workspace.ecommerce.ecommerce_data_2019_nov\")\n",
    "    print(f\"Version: {v} | Shape: ({df_version.count()}, {len(df_version.columns)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6d1e339-e322-4bd9-a579-18c212304765",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Optimize tables"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>files_added</th><th>files_removed</th><th>size_added</th><th>size_removed</th></tr></thead><tbody><tr><td>0</td><td>0</td><td>0</td><td>0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         0,
         0,
         0,
         0
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "files_added",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "files_removed",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "size_added",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "size_removed",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Optimize the Delta table to improve performance\n",
    "optimized_result = spark.sql(\"OPTIMIZE workspace.ecommerce.ecommerce_data_2019_nov ZORDER BY (event_time, event_type, product_id, category_id, category_code, brand, price, user_id, user_session)\")\n",
    "# optimized_result.show(truncate=False).display()\n",
    "from pyspark.sql.functions import col\n",
    "optimized_result.select(\n",
    "    col(\"metrics.numFilesAdded\").alias(\"files_added\"),\n",
    "    col(\"metrics.numFilesRemoved\").alias(\"files_removed\"),\n",
    "    col(\"metrics.filesAdded.totalSize\").alias(\"size_added\"),\n",
    "    col(\"metrics.filesRemoved.totalSize\").alias(\"size_removed\")\n",
    ").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31d22bb4-0fec-4d21-bc2a-fd7aefc2e7d6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Clean old files"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[path: string]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean up old files from the Delta table to reclaim storage\n",
    "spark.sql(\"VACUUM workspace.ecommerce.ecommerce_data_2019_nov RETAIN 168 HOURS\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Databricks 14-Day AI Challenge - Day 5",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
